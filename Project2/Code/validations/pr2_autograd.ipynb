{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import grad\n",
    "from autograd import elementwise_grad\n",
    "import autograd.numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuralnetwork_w_softder\n",
    "from neuralnetwork_w_softder import Sigmoid\n",
    "from neuralnetwork_w_softder import RELU\n",
    "from neuralnetwork_w_softder import LRELU\n",
    "from neuralnetwork_w_softder import Softmax\n",
    "\n",
    "from neuralnetwork_w_softder import MSE\n",
    "from neuralnetwork_w_softder import BCE_logloss\n",
    "from neuralnetwork_w_softder import MCE_multiclass\n",
    "import math\n",
    "import sys\n",
    "import warnings\n",
    "from random import random, seed\n",
    "from copy import deepcopy, copy\n",
    "from typing import Tuple, Callable\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing autograd with the manual written derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data we are testing for the activation\n",
    "inputs = np.array([0.2, 0.5, 0.8, 1.0, 1.5]).reshape(-1, 1)\n",
    "\n",
    "#The data we are testing for cost-functions\n",
    "targets = np.array([0.25, 0.5, 0.75, 1.0]).reshape(-1, 1)\n",
    "predict = np.array([0.22, 0.54, 0.77, 1.1]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the derivatives with the manual written code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.015      -0.04370629 -0.28409091]\n",
      " [ 0.02        0.04025765 -0.23148148]\n",
      " [ 0.01        0.02823264 -0.24350649]\n",
      " [ 0.05       -0.22727273 -0.22727273]]\n"
     ]
    }
   ],
   "source": [
    "MSE_deri = MSE()\n",
    "MSE_deri.derive = True\n",
    "MSE_deri(predict, targets, weights=None, lmd=0)\n",
    "\n",
    "cost_store = np.zeros((4,3))\n",
    "cost_list = [MSE(), BCE_logloss(), MCE_multiclass()]\n",
    "for i,cost in enumerate(cost_list):\n",
    "    cost_func = cost\n",
    "    cost_func.derive = True\n",
    "    cost_store[:,i] = cost_func(predict, targets, weights=None, lmd=0).ravel()\n",
    "\n",
    "print(cost_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd (with the functions needed - werent able to use autograd on classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "np.random.seed(123)\n",
    "def CostOLS(target):\n",
    "    \n",
    "    def func(X):\n",
    "        return (1.0 / target.shape[0]) * np.sum((target - X) ** 2)\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "def CostLogReg(target):\n",
    "\n",
    "    def func(X):\n",
    "        \n",
    "        return -(1.0 / target.shape[0]) * np.sum(\n",
    "            (target * np.log(X + 10e-10)) + ((1 - target) * np.log(1 - X + 10e-10))\n",
    "        )\n",
    "    return func \n",
    "\n",
    "\n",
    "def CostCrossEntropy(target):\n",
    "    \n",
    "    def func(X):\n",
    "        return -(1.0 / target.size) * np.sum(target * np.log(X + 10e-10))\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.015      -0.04370629 -0.28409091]\n",
      " [ 0.02        0.04025765 -0.23148148]\n",
      " [ 0.01        0.02823264 -0.24350649]\n",
      " [ 0.05       -0.22727273 -0.22727273]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/birkskogsrud/Desktop/prog_host25/.venv/lib/python3.13/site-packages/autograd/tracer.py:54: RuntimeWarning: invalid value encountered in log\n",
      "  return f_raw(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "func_list = [CostOLS, CostLogReg, CostCrossEntropy]\n",
    "auto_store = np.zeros((4,3))\n",
    "for i,cost_ in enumerate(func_list):\n",
    "    func_list = cost_\n",
    "    func_der = grad(func_list(targets))\n",
    "    auto_store[:,i] = func_der(predict).ravel()\n",
    "\n",
    "print(auto_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check if the manually written derivatives of the cost functions are equal to the autograd derivatives. The tolerance when using np.allclose is automatically chosen to 10^(-6), and all the values are larger than 0.01 so this tolerance is good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Juhu! The derivatives are correct compared with autograd:)\n"
     ]
    }
   ],
   "source": [
    "if np.allclose(cost_store, auto_store):\n",
    "    print('Juhu! The derivatives are correct compared with autograd:)')\n",
    "else:\n",
    "    print('Buhu! The derivatives are not equal:(')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onto the activation functions:\n",
    "\n",
    "(Not softmax as we are using the combined derivative of softmax and cross-entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.24751657 1.         1.        ]\n",
      " [0.23500371 1.         1.        ]\n",
      " [0.2139097  1.         1.        ]\n",
      " [0.19661193 1.         1.        ]\n",
      " [0.14914645 1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "activations_list = [Sigmoid(), RELU(), LRELU()]\n",
    "act_der = np.zeros((5,3))\n",
    "\n",
    "for i,act in enumerate(activations_list):\n",
    "    activation = act\n",
    "    activation.derive = True\n",
    "    act_der[:,i] = activation(inputs).ravel()\n",
    "\n",
    "print(act_der)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    try:\n",
    "        return 1.0 / (1 + np.exp(-X))\n",
    "    except FloatingPointError:\n",
    "        return np.where(X > np.zeros(X.shape), np.ones(X.shape), np.zeros(X.shape))\n",
    "\n",
    "\n",
    "def softmax(X):\n",
    "    X = X - np.max(X, axis=-1, keepdims=True)\n",
    "    delta = 10e-10\n",
    "    return np.exp(X) / (np.sum(np.exp(X), axis=-1, keepdims=True) + delta)\n",
    "\n",
    "\n",
    "def RELU(X):\n",
    "    return np.where(X > np.zeros(X.shape), X, np.zeros(X.shape))\n",
    "\n",
    "\n",
    "def LRELU(X):\n",
    "    delta = 10e-4\n",
    "    return np.where(X > np.zeros(X.shape), X, delta * X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.24751657 1.         1.        ]\n",
      " [0.23500371 1.         1.        ]\n",
      " [0.2139097  1.         1.        ]\n",
      " [0.19661193 1.         1.        ]\n",
      " [0.14914645 1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "auto_act = [sigmoid, RELU, LRELU]\n",
    "act_store = np.zeros((5,3))\n",
    "for i,act_ in enumerate(auto_act):\n",
    "    act_grad = elementwise_grad(act_)\n",
    "    act_store[:,i] = act_grad(inputs).ravel()\n",
    "\n",
    "print(act_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check if the manually written derivatives of the activation functions are equal to the autograd derivatives. The tolerance when using np.allclose is automatically chosen to 10^(-6), and all the values are larger than 0.1 so this tolerance is good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Juhu! The derivatives are correct compared with autograd:)\n"
     ]
    }
   ],
   "source": [
    "if np.allclose(act_der, act_store):\n",
    "    print('Juhu! The derivatives are correct compared with autograd:)')\n",
    "else:\n",
    "    print('Buhu! The derivatives are not equal:(')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
